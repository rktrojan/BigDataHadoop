

================================
//pseudo-code

================================
//pseudo-code - RDD
f1 = sc.textFile('emp.csv').map(lambda x:(x,1)).collect()
================================
//pseudo-code - DataFrame/Datasets
b = sc.sqlcontext.read_csv('emp.csv')


c= sc.toDF(a)
================================
-----------------------------------------------------------

================================
spark RDD

collection --- sc.parallize(data)

file ----   sc.textFile('emp.csv')


===================
spark DF

collection --- sqlContext.createDataFrame()

file --- 	sqlContext.read.csv()
			sqlContext.read.text()
			
			
			DSL and SQL
	

================================
-----------------------------------------------------------

https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations
https://spark.apache.org/docs/2.3.0/sql-programming-guide.html#running-sql-queries-programmatically
